{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6c13e02",
   "metadata": {},
   "source": [
    "# Parameterization\n",
    "\n",
    "\n",
    "Often times notebooks will need to execute by injecting variables and values from externally to the notebook. This might be service credentials, specific endpoints, variations for the notebook (e.g. if a notebook is executing a hyperparameter grid search, they may be executed many times in parallel but only with external variables changing). Today, there are two primary ways that notebooks are parameterized - via new tool (like Papermill) or with code generation directly into the notebook (e.g. regex replacement against fields below). \n",
    "\n",
    "__Today:__ Values are just hard coded in, and require manual changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aee8a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "data_source = \"http://data.contoso.com/blob/important_data.csv\"\n",
    "postgresql_credentials = \"\"\n",
    "test_cert_store_root = \"/var/opt/secrets/test-certificates\" # maps to local file system, not uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7f1574",
   "metadata": {},
   "source": [
    "# Lack Environment Description\n",
    "\n",
    "Import behavior is always one of the most challenging (and most common to get wrong). Most commonly, people will import several packages at the top of a file. They are unlikely to include specific versions and may use structures which are hard to introspect (e.g. 'from foo import bar as qaz'). However, the libraries may be imported outside the notebook itself, via inline bash commands or via a command line (Jupyter notebooks execute inside the command line environment - so if packages were imported there, the notebook will run normally). This will often lead to mismatched environments when the package is deployed to another environment or containerized. Because the time in deployment of complex pipelines is so long, this could be 10 minutes or more before noticing that something is wrong.\n",
    "\n",
    "__Today:__ Requires the packages are already installed (lack version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee26d589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d873fb00",
   "metadata": {},
   "source": [
    "# Use of Inline Local Commands\n",
    "\n",
    "In addition to setting up complicated imports, often data scientists will do inline bash to get (or change) system information. This is particularly pernitious because it is hard to introspect, capture errors and may change the environment altogether.\n",
    "\n",
    "__Today__: Use an escape sequence to execute locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca064c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade pip\n",
    "!python -m pip install --upgrade pip\n",
    "\n",
    "# Install a package\n",
    "!pip install tensorflow\n",
    "\n",
    "# Execute a command using a CLI - manually parsing output, regexing for errors, and using P.wait()\n",
    "wait = True \n",
    "try:\n",
    "    if no_output:\n",
    "        p = Popen(cmd_actual)\n",
    "    else:\n",
    "        p = Popen(cmd_actual, stdout=PIPE, stderr=PIPE, bufsize=1)\n",
    "        with p.stdout:\n",
    "            for line in iter(p.stdout.readline, b''):\n",
    "                line = line.decode()\n",
    "                if return_output:\n",
    "                    output = output + line\n",
    "                else:\n",
    "                    if cmd.startswith(\"azdata notebook run\"): # Hyperlink the .ipynb file\n",
    "                        regex = re.compile('  \"(.*)\"\\: \"(.*)\"') \n",
    "                        match = regex.match(line)\n",
    "                        if match:\n",
    "                            if match.group(1).find(\"HTML\") != -1:\n",
    "                                display(Markdown(f' - \"{match.group(1)}\": \"{match.group(2)}\"'))\n",
    "                            else:\n",
    "                                display(Markdown(f' - \"{match.group(1)}\": \"[{match.group(2)}]({match.group(2)})\"'))\n",
    "\n",
    "                                wait = False\n",
    "                                break # otherwise infinite hang, have not worked out why yet.\n",
    "                    else:\n",
    "                        print(line, end='')\n",
    "\n",
    "    if wait:\n",
    "        p.wait()\n",
    "except FileNotFoundError as e:\n",
    "    raise FileNotFoundError(f\"Executable '{cmd_actual[0]}' not found in path (where/which)\") from e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47495e3",
   "metadata": {},
   "source": [
    "# Exception Handling and Live Tracing\n",
    "\n",
    "One of the most powerful components of Jupyter notebooks is the ability to both print out inputs and outputs in between cells as a tool to debug and understand what is going on. However, as notebooks move to production, things like exceptions and print outs can get lost because the standard method of user understanding is with non-loggable tools. With standard observability, this is problematic, but with errors (particularly unhandled ones) this can lead to business critical reliability issues.\n",
    "\n",
    "__Today__: Use printf to understand processes, stdout for raising exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9350fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loading a Kubernetes client for use\n",
    "import os\n",
    "from IPython.display import Markdown\n",
    "\n",
    "# ISSUE: Try-except block for loading libraries\n",
    "try:\n",
    "    from kubernetes import client, config\n",
    "    from kubernetes.stream import stream\n",
    "except ImportError: \n",
    "\n",
    "    # Install the Kubernetes module\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install kubernetes    \n",
    "    \n",
    "    try:\n",
    "        from kubernetes import client, config\n",
    "        from kubernetes.stream import stream\n",
    "    except ImportError:\n",
    "        display(Markdown(f'HINT: Use [SOP059 - Install Kubernetes Python module](../install/sop059-install-kubernetes-module.ipynb) to resolve this issue.'))\n",
    "        raise\n",
    "\n",
    "if \"KUBERNETES_SERVICE_PORT\" in os.environ and \"KUBERNETES_SERVICE_HOST\" in os.environ:\n",
    "    config.load_incluster_config()\n",
    "else:\n",
    "    # ISSUE: Try-except block for loading Kubeconfig\n",
    "    try:\n",
    "        config.load_kube_config()\n",
    "    except:\n",
    "        display(Markdown(f'HINT: Use [TSG118 - Configure Kubernetes config](../repair/tsg118-configure-kube-config.ipynb) to resolve this issue.'))\n",
    "        raise\n",
    "\n",
    "# ISSUE: No try-except block for loading the API (could fail silently)\n",
    "api = client.CoreV1Api()\n",
    "\n",
    "# ISSUE: Print out for notifying that client loaded correctly (should be auto-logged/traceable)\n",
    "print('Kubernetes client instantiated')\n",
    "\n",
    "# Idea - check or wrap? See AST syntax tracing here - https://engineering.soroco.com/abstract-syntax-tree-for-patching-code-and-assessing-code-quality/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9aeaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISSUE: Functions and tracing often cross cells\n",
    "def important_math(a, b):\n",
    "    return (a * 10, b / 20)\n",
    "\n",
    "a = 20\n",
    "b = 40\n",
    "print(f\"A: {a}\")\n",
    "print(f\"B: {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c8b564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISSUE: Second cell for execution - works when executing monolithically, does not work when cells auto-split\n",
    "\n",
    "a, b = important_math(a, b)\n",
    "\n",
    "print(f\"A: {a}\")\n",
    "print(f\"B: {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85192191",
   "metadata": {},
   "source": [
    "# Not built to execute headlessly or declaratively\n",
    "In addition to the parameterization at top, often times scripts will be written to either interactively pick up input from users during the run (via inputs or via changing parameters), and not built to execute idempotently. This means that notebooks can be dropped into automation automatically without rewriting significant portions of code - and often without tests. Ideally, the system would be more aware of the environment it was being executed in and could have overrides (via environment variables, file injection) for inputs, and \"execute once\" as first class objects.\n",
    "\n",
    "Further, especially when executing headlessly, a very common pattern is to just poll indefinitely until it fails - further poor use in automation environments.\n",
    "\n",
    "__Today__: Fragile notebooks that have to be hand run and checked at each step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d0950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISSUE: Inject values for a given function via hand changed variables\n",
    "import re\n",
    "\n",
    "pod = None # All\n",
    "container = \"app-service-proxy\"\n",
    "expressions_to_analyze = [\n",
    "    re.compile(\".{23}[error]\")\n",
    "]\n",
    "\n",
    "\n",
    "RESOURCE_GROUP_NAME = 'SampleDB-Resource-Group'\n",
    "LOCATION = \"westus\"\n",
    "\n",
    "resource_client = get_client_from_cli_profile(ResourceManagementClient)\n",
    "rg_result = resource_client.resource_groups.create_or_update(RESOURCE_GROUP_NAME,\n",
    "    { \"location\": LOCATION })\n",
    "\n",
    "# ISSUE: More magic constants\n",
    "db_server_name = os.environ.get(\"DB_SERVER_NAME\", f\"SampleDB-MySQL-{random.randint(1,100000):05}\")\n",
    "db_admin_name = os.environ.get(\"DB_ADMIN_NAME\", \"azureuser\")\n",
    "db_admin_password = os.environ.get(\"DB_ADMIN_PASSWORD\", \"ChangePa$$w0rd24\")\n",
    "\n",
    "# ISSUE: Creation of resources without checking to see if they have already been created (relying on the service to fail nicely or not create duplicates)\n",
    "import random, os\n",
    "from azure.common.client_factory import get_client_from_cli_profile\n",
    "from azure.mgmt.resource import ResourceManagementClient\n",
    "\n",
    "from azure.mgmt.rdbms.mysql import MySQLManagementClient\n",
    "from azure.mgmt.rdbms.mysql.models import ServerForCreate, ServerPropertiesForDefaultCreate, ServerVersion\n",
    "\n",
    "# ISSUE: No checking to see if the client was provisioned properly, or raised exceptions\n",
    "mysql_client = get_client_from_cli_profile(MySQLManagementClient)\n",
    "\n",
    "# ISSUE: Provision the server and wait for the result - just polling\n",
    "poller = mysql_client.servers.create(RESOURCE_GROUP_NAME,\n",
    "    db_server_name,\n",
    "    ServerForCreate(\n",
    "        location=LOCATION,\n",
    "        properties=ServerPropertiesForDefaultCreate(\n",
    "            administrator_login=db_admin_name,\n",
    "            administrator_login_password=db_admin_password,\n",
    "            version=ServerVersion.five_full_stop_seven\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "server = poller.result()\n",
    "\n",
    "# ISSUE: Block of code for addition services (firewall and database provisioning) - no global concept of complete or undo\n",
    "\n",
    "RULE_NAME = \"allow_ip\"\n",
    "\n",
    "# ISSUE: Relying on os specifics for IP address \n",
    "ip_address = os.environ[\"PUBLIC_IP_ADDRESS\"]\n",
    "\n",
    "\n",
    "poller = mysql_client.firewall_rules.create_or_update(RESOURCE_GROUP_NAME,\n",
    "    db_server_name, RULE_NAME,\n",
    "    ip_address,  # Start ip range\n",
    "    ip_address   # End ip range\n",
    ")\n",
    "firewall_rule = poller.result()\n",
    "\n",
    "# ISSUE: Another magic constant, inline with code\n",
    "DB_NAME = \"example-db1\"\n",
    "\n",
    "# ISSUE: Another polling example, no structured reporting\n",
    "poller = mysql_client.databases.create_or_update(RESOURCE_GROUP_NAME,\n",
    "    db_server_name, DB_NAME)\n",
    "\n",
    "db_result = poller.result()\n",
    "print(f\"Provisioned MySQL database {db_result.name} with ID {db_result.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bc02e1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7b6c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No caching for common functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297d9b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No parallel execution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e2ebab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run step statelessly with High Mem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ca0880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run step with GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a7a4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out of order execution (to prevent blocking) - DAG follows from step early on (skipping a bunch of long running steps)\n",
    "\n",
    "cmd = \"\"\"echo \"CPU %\\t MEM %\\t MEM\\t PROCESS\" &&\n",
    "ps aux |\n",
    "awk '\n",
    "    {mem[$11] += int($6/1024)};\n",
    "    {cpuper[$11] += $3};\n",
    "    {memper[$11] += $4};\n",
    "END {\n",
    "    for (i in mem) {\n",
    "        print cpuper[i] \"%\\t\", memper[i] \"%\\t\", mem[i] \"MB\\t\", i\n",
    "    }\n",
    "}' |\n",
    "sort -k3nr\n",
    "\"\"\"\n",
    "\n",
    "pod_list = api.list_namespaced_pod(namespace)\n",
    "pod_names = [pod.metadata.name for pod in pod_list.items]\n",
    "\n",
    "for pod in pod_list.items:\n",
    "    container_names = [container.name for container in pod.spec.containers]\n",
    "\n",
    "    for container in container_names:\n",
    "        print (f\"CONTAINER: {container} / POD: {pod.metadata.name}\")\n",
    "        try:\n",
    "            print(stream(api.connect_get_namespaced_pod_exec, pod.metadata.name, namespace, command=['/bin/sh', '-c', cmd], container=container, stderr=True, stdout=True))\n",
    "        except Exception:\n",
    "            print (f\"Failed to get CPU/Memory for container: {container} in POD: {pod.metadata.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2063349a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No retry automation for external service\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0b6a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to common storage location (with metadata) - future debugging, and means everything here can be torn down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c30ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "52dea00a7bb867e5b30ba315b0d76d240cbdf1f79ef38a0bce87e51983d52c51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
