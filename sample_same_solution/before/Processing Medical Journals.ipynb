{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2f950e1",
   "metadata": {},
   "source": [
    "# Background \n",
    "A very common pattern in data analysis is to do \"Named Entity Recognition\". This is where a team builds a model to idenify key elements out of text for later processing. For example:\n",
    "\n",
    "“Mark Zuckerberg is one of the founders of Facebook, a company from the United States” \n",
    "\n",
    "Out of this sentance, a model may produce:\n",
    "- \"Mark Zuckerberg\" - Two word phrase describing a person\n",
    "- \"Facebook\" - Company, related to Mark Zuckerberg in some way\n",
    "- \"United States\" - Location, related to Mark Zuckerberg & Facebook in some way\n",
    "\n",
    "More advanced models could draw some further association using nouns and verbs, but even this level of analysis offers a good deal of information to search engines, content correlation, categorization, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5123d8ea",
   "metadata": {},
   "source": [
    "# Doing Named Entity Recognition on Medical Journals\n",
    "\n",
    "A new effort for automated analysis of text is using Named Entity Recognition techniques on existing and newly published content to enable faster analysis. In areas where there are too many publications for humans to read regularly, this unlocks far more knowledge quickly.\n",
    "\n",
    "For example, in medical journals, many thousands of articles are published every month, and there are over 25 million historical papers as well - far more than any organization could process. Using a named entity recognition tool will enable fast searching, analysis of associated papers and potential information for further investigation.\n",
    "\n",
    "The example we will look at is a simplified version of this one - \n",
    "- https://nbviewer.jupyter.org/github/andressotov/Named-Entity-Recognition-in-French-biomedical-text/blob/master/Named%20Entity%20Recognition%20in%20French%20biomedical%20text.ipynb \n",
    "- https://nbviewer.jupyter.org/github/andressotov/Named-Entity-Recognition-in-French-biomedical-text/blob/master/Named%20Entity%20Recognition%20in%20French%20biomedical%20text%20Part%202.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044de8e3",
   "metadata": {},
   "source": [
    "# Step 1 - Ingest the data from flat files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "366f09f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Glutamine synthétase et glutamine gamma-glutamyl transferase dans le rein de l' homme , du chien et du rat\\n\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path to the data train set \n",
    "path_train = \"corpus/dev/MEDLINE\"\n",
    "\n",
    "def read_file(path,file,ext):    \n",
    "    f = path+'/'+file+ext\n",
    "    with open(f, 'rt', encoding='utf-8') as myfile:\n",
    "        data = myfile.readlines()\n",
    "    return data\n",
    "\n",
    "# To read a file and obtain its content\n",
    "data = read_file(path_train,\"2787\",\".txt\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8411786a",
   "metadata": {},
   "source": [
    "# Step 2 - Print out the data to make sure it's being read properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "544d5344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T1\\tCHEM 0 20\\tGlutamine synthétase\\n',\n",
      " '#1\\tAnnotatorNotes T1\\tC0017801\\n',\n",
      " 'T2\\tCHEM 0 9\\tGlutamine\\n',\n",
      " '#2\\tAnnotatorNotes T2\\tC0017797\\n',\n",
      " 'T3\\tCHEM 10 20\\tsynthétase\\n',\n",
      " '#3\\tAnnotatorNotes T3\\tC0023689\\n',\n",
      " 'T4\\tCHEM 24 33\\tglutamine\\n',\n",
      " '#4\\tAnnotatorNotes T4\\tC0017797\\n',\n",
      " 'T5\\tCHEM 34 60\\tgamma-glutamyl transferase\\n',\n",
      " '#5\\tAnnotatorNotes T5\\tC0017040\\n',\n",
      " 'T6\\tCHEM 49 60\\ttransferase\\n',\n",
      " '#6\\tAnnotatorNotes T6\\tC0040676\\n',\n",
      " 'T7\\tANAT 69 73\\trein\\n',\n",
      " '#7\\tAnnotatorNotes T7\\tC0022646\\n',\n",
      " 'T8\\tLIVB 80 85\\thomme\\n',\n",
      " '#8\\tAnnotatorNotes T8\\tC0086418\\n',\n",
      " 'T9\\tLIVB 91 96\\tchien\\n',\n",
      " '#9\\tAnnotatorNotes T9\\tC0012984\\n',\n",
      " 'T10\\tLIVB 103 106\\trat\\n',\n",
      " '#10\\tAnnotatorNotes T10\\tC0034721\\n']\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "data = read_file(path_train,\"2787\",\".ann\")\n",
    "pprint.pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cbeebc",
   "metadata": {},
   "source": [
    "# Step 3 - Convert from .ann File Type to a Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4baf8595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'T1': {'label': ['CHEM', '0', '20'], 'text': 'Glutamine synthétase'},\n",
      " 'T10': {'label': ['LIVB', '103', '106'], 'text': 'rat'},\n",
      " 'T2': {'label': ['CHEM', '0', '9'], 'text': 'Glutamine'},\n",
      " 'T3': {'label': ['CHEM', '10', '20'], 'text': 'synthétase'},\n",
      " 'T4': {'label': ['CHEM', '24', '33'], 'text': 'glutamine'},\n",
      " 'T5': {'label': ['CHEM', '34', '60'], 'text': 'gamma-glutamyl transferase'},\n",
      " 'T6': {'label': ['CHEM', '49', '60'], 'text': 'transferase'},\n",
      " 'T7': {'label': ['ANAT', '69', '73'], 'text': 'rein'},\n",
      " 'T8': {'label': ['LIVB', '80', '85'], 'text': 'homme'},\n",
      " 'T9': {'label': ['LIVB', '91', '96'], 'text': 'chien'}}\n"
     ]
    }
   ],
   "source": [
    "def ann_text2dict(lines):\n",
    "    d = {}\n",
    "    for l in lines:\n",
    "        if not l.startswith('#'):\n",
    "            t = l.split('\\t')\n",
    "            if ';' in t[1]:\n",
    "                t[1] = t[1].replace(';',' ; ')\n",
    "            d[t[0]] = {\n",
    "                'label':t[1].split(' '),\n",
    "                'text':t[2].replace('\\n','')\n",
    "            }\n",
    "    return d\n",
    "d = ann_text2dict(data)\n",
    "pprint.pprint(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b9a47f",
   "metadata": {},
   "source": [
    "# Step 4 - Combine .txt and .ann files into a Single Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea446d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def save_pickle(data,file):\n",
    "    pick_file = open(file+\".pkl\", \"wb\")\n",
    "    pickle.dump(data, pick_file)\n",
    "    pick_file.close()\n",
    "\n",
    "def load_pickle(file):\n",
    "    pick_file = open(file+\".pkl\", \"rb\")\n",
    "    data = pickle.load(pick_file)\n",
    "    pick_file.close()\n",
    "    return data\n",
    "\n",
    "def collect_files(path,set):\n",
    "    dirs = os.listdir(path)\n",
    "    ltxt = []\n",
    "    lann = []\n",
    "    for f in dirs:\n",
    "        if f.endswith('.txt'):\n",
    "            f = f.replace('.txt','')\n",
    "            ltxt.append(f)\n",
    "        elif f.endswith('.ann'):\n",
    "            f = f.replace('.ann','')\n",
    "            lann.append(f)\n",
    "        else:\n",
    "            pass\n",
    "    save_pickle(ltxt,set+'_txt')\n",
    "    save_pickle(lann,set+'_ann')\n",
    "    \n",
    "collect_files(path_train,'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4aec8b",
   "metadata": {},
   "source": [
    "# Step 5 - Process the Text in .ann Files into a Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4e0ff63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of ann files 832\n"
     ]
    }
   ],
   "source": [
    "def ann_files2dict(pic_file,path,set):\n",
    "    lann = load_pickle(pic_file)\n",
    "    lnew = []\n",
    "    c = 0\n",
    "    for ann in lann:\n",
    "        data = read_file(path, ann, \".ann\")\n",
    "        dic = ann_text2dict(data)\n",
    "        c+=1\n",
    "        lnew.append(dic)\n",
    "\n",
    "    save_pickle(lnew,set+'_ann_dics')\n",
    "    return lnew\n",
    "lnew = ann_files2dict('train_ann',path_train,'train')\n",
    "\n",
    "print(\"# of ann files\",len(lnew))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e04654",
   "metadata": {},
   "source": [
    "# Step 6 - Reduce Number of Tokens Per Label (Should Only Be One)\n",
    "\n",
    "There is one situation that we didn’t mentioned before: it is possible that more labels are assigned to the same token (annotations overlap). In this case, we will only choose one of them and discard the other. For example, let’s assume that we have the following text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f1030fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'T1': {'label': ['CHEM', '0', '20'], 'text': 'Glutamine synthétase'},\n",
      " 'T10': {'label': ['LIVB', '103', '106'], 'text': 'rat'},\n",
      " 'T2': {'label': ['CHEM', '0', '9'], 'text': 'Glutamine'},\n",
      " 'T3': {'label': ['CHEM', '10', '20'], 'text': 'synthétase'},\n",
      " 'T4': {'label': ['CHEM', '24', '33'], 'text': 'glutamine'},\n",
      " 'T5': {'label': ['CHEM', '34', '60'], 'text': 'gamma-glutamyl transferase'},\n",
      " 'T6': {'label': ['CHEM', '49', '60'], 'text': 'transferase'},\n",
      " 'T7': {'label': ['ANAT', '69', '73'], 'text': 'rein'},\n",
      " 'T8': {'label': ['LIVB', '80', '85'], 'text': 'homme'},\n",
      " 'T9': {'label': ['LIVB', '91', '96'], 'text': 'chien'}}\n"
     ]
    }
   ],
   "source": [
    "def is_subrange(r1,r2):\n",
    "    [a1,b1] = r1\n",
    "    [a2,b2] = r2\n",
    "    if int(a2) <= int(a1) and int(b1) <= int(b2): # [a1,b1] subrange of [a2,b2]\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def contained(key, ann_dic):\n",
    "    piv = ann_dic[key]['label']\n",
    "    for k in ann_dic.keys():\n",
    "        if not k == key:\n",
    "            lab_field = ann_dic[k]['label']\n",
    "            if len(lab_field) == 3:\n",
    "                if len(piv) == 3:\n",
    "                    if is_subrange([piv[1],piv[2]],\n",
    "                                   [lab_field[1],lab_field[2]]):\n",
    "                        return True\n",
    "    return False\n",
    "\n",
    "def remove_contained(ann_dic):\n",
    "    lrem = []\n",
    "    for k in ann_dic.keys():\n",
    "        if contained(k,ann_dic):\n",
    "            lrem.append(k)\n",
    "    for i in lrem:\n",
    "        del ann_dic[i]\n",
    "    return ann_dic\n",
    "\n",
    "pprint.pprint(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5eaa1502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'T1': {'label': ['CHEM', '0', '20'], 'text': 'Glutamine synthétase'},\n",
      " 'T10': {'label': ['LIVB', '103', '106'], 'text': 'rat'},\n",
      " 'T4': {'label': ['CHEM', '24', '33'], 'text': 'glutamine'},\n",
      " 'T5': {'label': ['CHEM', '34', '60'], 'text': 'gamma-glutamyl transferase'},\n",
      " 'T7': {'label': ['ANAT', '69', '73'], 'text': 'rein'},\n",
      " 'T8': {'label': ['LIVB', '80', '85'], 'text': 'homme'},\n",
      " 'T9': {'label': ['LIVB', '91', '96'], 'text': 'chien'}}\n"
     ]
    }
   ],
   "source": [
    "d1 = remove_contained(d)\n",
    "pprint.pprint(d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f01283b",
   "metadata": {},
   "source": [
    "# Step 7 - Verify the Reduction is Valid By Examining Non-Continuous Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99bdd40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set train\n",
      "Number of non continuous segments 21 % 0.705408128988915\n",
      "Total number of segments 2977\n"
     ]
    }
   ],
   "source": [
    "def cont_ncont(ann_dic):\n",
    "    nnc = 0 # number of non continuous segments\n",
    "    ntot = 0 # total number of segments\n",
    "    for k in ann_dic.keys():\n",
    "        piv = ann_dic[k]['label']\n",
    "        ntot+=1\n",
    "        if not len(piv) == 3:\n",
    "            nnc+=1\n",
    "    return [nnc,ntot]\n",
    "\n",
    "\n",
    "def count_non_continuous(set):\n",
    "    ldics = load_pickle(set + '_ann_dics')\n",
    "    cnc = 0 # cont non continuous segments\n",
    "    ctot = 0 # cont total segments\n",
    "    for i in range(len(ldics)):\n",
    "        [nnc,ntot] = cont_ncont(ldics[i])\n",
    "        cnc+=nnc\n",
    "        ctot+=ntot\n",
    "    print(\"set\",set)\n",
    "    print(\"Number of non continuous segments\",cnc,'%',(cnc/ctot)*100)\n",
    "    print(\"Total number of segments\",ctot)\n",
    "count_non_continuous('train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0db38ed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24128b82",
   "metadata": {},
   "source": [
    "# Step 8 - Transform the Imported Data into the Right Format for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89bdf6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'CHEM', 'range': ['0', '20'], 'text': 'Glutamine synthétase'},\n",
      " {'label': 'CHEM', 'range': ['24', '33'], 'text': 'glutamine'},\n",
      " {'label': 'CHEM', 'range': ['34', '60'], 'text': 'gamma-glutamyl transferase'},\n",
      " {'label': 'ANAT', 'range': ['69', '73'], 'text': 'rein'},\n",
      " {'label': 'LIVB', 'range': ['80', '85'], 'text': 'homme'},\n",
      " {'label': 'LIVB', 'range': ['91', '96'], 'text': 'chien'},\n",
      " {'label': 'LIVB', 'range': ['103', '106'], 'text': 'rat'}]\n"
     ]
    }
   ],
   "source": [
    "def simple_dic(ann_dic):\n",
    "    lista = []\n",
    "    for t in ann_dic.keys():\n",
    "        pt = ann_dic[t]\n",
    "        dic = {\n",
    "            'label' : pt['label'][0],\n",
    "            'range' : pt['label'][1:],\n",
    "            'text' : pt['text']\n",
    "        }\n",
    "        lista.append(dic)\n",
    "    return lista\n",
    "sdic = simple_dic(d1)\n",
    "pprint.pprint(sdic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d811e50f",
   "metadata": {},
   "source": [
    "# Step 9 - Augment the Data with Additional Information from the Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2eaa144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ann_dic': [{'label': 'PROC', 'range': ['3', '15'], 'text': 'rhinoplastie'},\n",
      "             {'label': 'PROC',\n",
      "              'range': ['18', '34'],\n",
      "              'text': 'soins infirmiers'},\n",
      "             {'label': 'PROC',\n",
      "              'range': ['18', '38', ';', '46', '57'],\n",
      "              'text': 'soins infirmiers pré opératoires'},\n",
      "             {'label': 'PROC',\n",
      "              'range': ['18', '34', ';', '42', '57'],\n",
      "              'text': 'soins infirmiers postopératoires'}],\n",
      " 'txt': [\"Indice radiographique de profil de l' impression basilaire .\\n\"]}\n"
     ]
    }
   ],
   "source": [
    "def mix_txt_ann(pic_file,path,set):\n",
    "    ltxt = load_pickle(pic_file)\n",
    "    lnew = []\n",
    "    for i in range(len(ltxt)):\n",
    "        data = read_file(path, ltxt[i], \".txt\")\n",
    "        ldics = load_pickle(set + '_ann_dics')\n",
    "        ann_dic = remove_contained(ldics[i])\n",
    "        ndic ={\n",
    "            'txt':data,\n",
    "            'ann_dic': simple_dic(ann_dic)\n",
    "        }\n",
    "        lnew.append(ndic)\n",
    "    save_pickle(lnew,set+'_txt_ann')\n",
    "set = 'train'\n",
    "mix_txt_ann('train_txt',path_train,set)\n",
    "lista = load_pickle(set+'_txt_ann')\n",
    "pprint.pprint(lista[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d7072d",
   "metadata": {},
   "source": [
    "# Step 10 - Convert Dicts Into List of Tuples, and Fill In Missing Tags with \"None\"\n",
    "\n",
    "We can simplify more this structure converting the list of dictionaries that corresponds to the annotation part in a list of tuples. We need also to tag all segments included in the TXT segments. We already have some of them tagged, but others don't. We will tag them as 'NONE' indicating that this tag is none of the others.\n",
    "\n",
    "The funstion ldic2ltup converts a list of dictionaries into a list of tuples, while the function complete_segments tag with 'NONE' the other non-tagged segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34abdadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ann_dic': [(0, 2, 'NONE', 'In'),\n",
      "             (3, 15, 'PROC', 'ice radiogra'),\n",
      "             (16, 17, 'NONE', 'h'),\n",
      "             (18, 34, 'PROC', 'que de profil de'),\n",
      "             (35, 60, 'NONE', \"l' impression basilaire .\")],\n",
      " 'txt': \"Indice radiographique de profil de l' impression basilaire .\\n\"}\n"
     ]
    }
   ],
   "source": [
    "def ldic2ltup(i,listai):\n",
    "    ann_dic = listai['ann_dic']\n",
    "    txt = listai['txt'][0]\n",
    "    ltup = []\n",
    "    for dic in ann_dic:\n",
    "        etiq = dic['label']\n",
    "        rango = dic['range']\n",
    "        if len(rango) < 3:\n",
    "            # print(\"<3\")\n",
    "            a = int(rango[0])\n",
    "            b = int(rango[1])\n",
    "            ltup.append((a, b, etiq, txt[a:b]))\n",
    "        #else:\n",
    "        #    print(\"varios tuplos\")\n",
    "    ltup.sort()\n",
    "    return ltup\n",
    "def complete_segments(set):\n",
    "    lista = load_pickle(set + '_txt_ann')\n",
    "    newl = []\n",
    "    for i in range(len(lista)):       # len(lista)):\n",
    "        txt = lista[i]['txt'][0] # a list with one elem\n",
    "        ltup = ldic2ltup(i, lista[i])\n",
    "        ltup.sort()\n",
    "        lt1 = []\n",
    "        if len(ltup) == 0:\n",
    "            tup = (0,len(txt),'NONE',txt)\n",
    "            lt1.append(tup)\n",
    "            continue\n",
    "        if ltup[0][0]>0:\n",
    "            a = 0\n",
    "            b = ltup[0][0]-1\n",
    "            tup = (a,b,'NONE',txt[a:b])\n",
    "            lt1.insert(0,tup)\n",
    "        for j in range(len(ltup)-1):\n",
    "            if ltup[j][1]+1 == ltup[j+1][0]: # consecutives\n",
    "                lt1.append(ltup[j])\n",
    "            else: # non consecutives\n",
    "                a = ltup[j][1]+1\n",
    "                lt1.append(ltup[j]) # previous one\n",
    "                b = ltup[j+1][0]-1\n",
    "                tup = (a,b,'NONE',txt[a:b])\n",
    "                lt1.append(tup) # new one\n",
    "        lt1.append(ltup[-1])\n",
    "        if ltup[-1][1] < len(txt):\n",
    "            a = ltup[-1][1]+1\n",
    "            b = len(txt) -1\n",
    "            tup = (a,b,'NONE',txt[a:b])\n",
    "            lt1.append(tup)\n",
    "        lt1.sort()\n",
    "        newl.append( {\n",
    "            'ann_dic' : lt1,\n",
    "            'txt' : txt\n",
    "        } )\n",
    "        save_pickle(newl,set + '_txt_ann2')\n",
    "set = 'train'\n",
    "complete_segments(set)\n",
    "new1 = load_pickle(set + '_txt_ann2')\n",
    "pprint.pprint(new1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0958e92a",
   "metadata": {},
   "source": [
    "# Step 11 - Tokenize Each Text Segment and Tag Each Token with Segment Tag\n",
    "\n",
    "With the help of function ldic2ltok_lab, we will tokenize each one of the text segments and tag each token with the corresponding tag i.e. the segment tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac4f90ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.6.2-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib\n",
      "  Using cached joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "Collecting regex\n",
      "  Using cached regex-2021.4.4-cp39-cp39-macosx_10_9_x86_64.whl (284 kB)\n",
      "Collecting click\n",
      "  Downloading click-8.0.1-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 19.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.61.1-py2.py3-none-any.whl (75 kB)\n",
      "\u001b[K     |████████████████████████████████| 75 kB 14.1 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.0.1 joblib-1.0.1 nltk-3.6.2 regex-2021.4.4 tqdm-4.61.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21968b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import RegexpTokenizer\n",
    "\n",
    "def ldic2ltok_lab(lsent):\n",
    "    ls_tok_lab = []\n",
    "    toknizer = RegexpTokenizer(r'''\\w'|\\w+|[^\\w\\s]''')\n",
    "    for sent in lsent:\n",
    "        ltup = sent['ann_dic']\n",
    "        lt_tok_lab = []\n",
    "        for (a, b, lab, txt) in ltup: # un segmento\n",
    "            lts = toknizer.tokenize(txt)\n",
    "            ltoks = [(t, lab) for t in lts]\n",
    "            lt_tok_lab.extend(ltoks)\n",
    "        ls_tok_lab.append(lt_tok_lab)\n",
    "    # print(ls_tok_lab)\n",
    "    return ls_tok_lab\n",
    "ls_tok_lab = ldic2ltok_lab(new1)\n",
    "save_pickle(ls_tok_lab,set + '_txt_ann3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dff3964",
   "metadata": {},
   "source": [
    "# Step 12 - Extract labels for each sentence\n",
    "Following we will explain the train and test part of the algorithm. In order to train the algorithm, we need first to obtain a vector y with the labels for each sentence and a matrix X with a set of features for each sentence.\n",
    "\n",
    "Let's begin explaining how to extract the labels for each sentence. The function sentences2labels converts the list of tuples (a,b,token,label) corresponding to the sentences into list of labels using the function sent2labels for extracting the labels for each sentence (i.e. list of tuples)\n",
    "\n",
    "**NOTE**: This step is not necessary if all being run as part of a single notebook. However, often times steps leading up to this one (e.g. importing, conversion, transformation) would be part of one system, and output (via pickles) into a second system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d4ef135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2labels(ltup):\n",
    "    liob = []\n",
    "    for (token,label) in ltup:\n",
    "        liob.append(label)\n",
    "    return liob\n",
    "\n",
    "def sentences2labels(ls_tok_lab):\n",
    "    print(\"extracting labels from sentences...\")\n",
    "    llabs = []\n",
    "    for ltup in list(ls_tok_lab):\n",
    "        labs = sent2labels(ltup)\n",
    "        llabs.append(labs)\n",
    "    return llabs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c50f8b4",
   "metadata": {},
   "source": [
    "By the other hand, we have the function sentences2features which converts a list of sentences into a list of lists of features (i.e. each feature correspond to a token and all the features that belong to a sentence are collected into the same list). It returns the list of lists of features. For each list of tuples (i.e. sentence), it uses the function word2features to convert the i-th element (token, label) of the tuple list into a set of features. It returns the set of features. In this example, the features considered were:\n",
    "\n",
    "Is the token lowercase or uppercase?\n",
    "Does the token begin with a capital?\n",
    "Is the token made up of digits?\n",
    "Is the token formed by alphanumeric chars?\n",
    "Is the token formed by alphabetic chars?\n",
    "the last 3 chars of the token\n",
    "the last 2 chars of the token This set of features is also collected for the previous and following token (if they exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0524a304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    token = sent[i][0]     # take word i from the sentence\n",
    "    features = {\n",
    "        # setup the features\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': token.lower(),  \t# Is the token lowercase?\n",
    "        'word.isupper()': token.isupper(),  # Is the token uppercase?\n",
    "        'word.istitle()': token.istitle(),  # Does the token begin with a capital?\n",
    "        'word.isdigit()': token.isdigit(),  # Is the token made up of digits?\n",
    "        'word.isalnum()': token.isalnum(),  # Is the token formed by alphanumeric chars?\n",
    "        'word.isalpha()': token.isalpha(),  # Is the token formed by alphabetic chars?\n",
    "        'word[-3:]': token[-3:],\t\t    # the last 3 chars of the token\n",
    "        'word[-2:]': token[-2:], \t        # the last 2 chars of the token\n",
    "    }\n",
    "    if i > 0:  # if it is not the first word\n",
    "        token1 = sent[i - 1][0]        # take the previous token\n",
    "        features.update({               # update the features\n",
    "            '-1:word.lower()': token1.lower(),      # Is the previous token lowercase?\n",
    "            '-1:word.isupper()': token1.isupper(),  # Is the previous token uppercase?\n",
    "            '-1:word.istitle()': token1.istitle(),  # Does it begin with a capital?\n",
    "            '-1:word.isdigit()': token1.isdigit(),  # Is the previous token made up of digits?\n",
    "            '-1:word.isalnum()': token1.isalnum(),  # Is the previous token formed by alphanumeric chars?\n",
    "            '-1:word.isalpha()': token1.isalpha(),  # Is the previous token formed by alphabetic chars?\n",
    "            '-1:word[-3:]': token1[-3:],            # the last 3 chars of the previous token\n",
    "            '-1:word[-2:]': token1[-2:],            # the last 2 chars of the previous token\n",
    "        })\n",
    "    else:       # if it is the first word\n",
    "        features['BOS'] = True  # set 'Begin Of Sentence'\n",
    "    if i < len(sent) - 1:           # if it is not the last word\n",
    "        token1 = sent[i + 1][0]     # take the next word\n",
    "        features.update({           # update the features:\n",
    "            '+1:word.lower()': token1.lower(),      # Is the next token lowercase?\n",
    "            '+1:word.istitle()': token1.istitle(),  # Does it begin with a capital?\n",
    "            '+1:word.isupper()': token1.isupper(),  # Is the it uppercase?\n",
    "            '+1:word.isdigit()': token1.isdigit(),  # Is the next token made up of digits?\n",
    "            '+1:word.isalnum()': token1.isalnum(),  # Is the next token formed by alphanumeric chars?\n",
    "            '+1:word.isalpha()': token1.isalpha(),  # Is the next token formed by alphabetic chars?\n",
    "            '+1:word[-3:]': token1[-3:],            # the last 3 chars of the next token\n",
    "            '+1:word[-2:]': token1[-2:],            # the last 2 chars of the next token\n",
    "        })\n",
    "    else:       # if it is the last word\n",
    "        features['EOS'] = True  # set 'End Of Sentence'\n",
    "    return features\n",
    "    \n",
    "def sentences2features(ls_tok_lab):\n",
    "    print(\"converting sentences to features...\")\n",
    "    lfeat = []\n",
    "    for ltup in ls_tok_lab:\n",
    "        lfeat.append([word2features(ltup, i) for i in range(len(ltup))])\n",
    "    return lfeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b018b8c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4e9471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29020b17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
